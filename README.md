# Тематическая классификация текстов      

<p align="justify">
Будем работать с датасетом для тематической классификации, "20 новостных групп", который состоит из примерно 20 тысяч сообщений электронной почты, распределенных по 20 категориям. Датасет состоит из двух частей — из обучающей выборки и тестовой. Суммарно 18 тысяч текстов, из них примерно две трети содержатся в обучающей выборке, а треть в — тестовой.  Классы в этом датасете обозначаются числами от 0 до 19.   
</p>

## Модель    

* Модель мешка слов       
* Логистическая регрессия       

<p align="justify">
Мы  преобразуем исходный текст в токены, затем построим словарь и взвесим токены. Реализуем модель мешка слов, разреженную векторную модель текста, затем мы реализуем логистическую регрессию на pytorch, обучим её, оценим её качество. А затем мы возьмём библиотеку scikit-learn (алгоритмы векторизации текстов из неё, а также реализацию логистической регрессии), обучим этот вариант модели и сравним с нашим вариантом. 
</p>    

##  Универсальные компоненты     

* Токенизация   
* Построение словаря    
* Фильтрация словаря   
* Построение матрицы   
* pytorch Dataset    
* цикл обучения и валидации    
* выбор лучшей модели в процессе обучения    
* оценка качества

## Основные функции     

<p align="justify">
<b>tokenize_corpus</b> - принимает на вход список текстов (список строк), а также функцию, которая будет принимать одну строку и возвращать список токенов. В функции с помощью регулярного выражения, описывается всё возможное множество токенов, которое нас интересует, затем короткие токины удаляются. По умолчанию задана минимальная длина токена: 4. Для описания множества допустимых токенов используется регулярное выражение. Это регулярное выражение срабатывает на непрерывных последовательностях букв или цифр. Таким образом, знаки препинания, пробелы, специальные символы игнорируются. После токенизации остаются только последовательности, состоящие из букв и цифр.   
</p>

<p align="justify">
<b>build_vocabulary</b> - принимает на вход список списков. Внешний список представляет собой датасет, а внутренние списки представляют отдельные документы и элементы внутренних списков — это токены в строковом виде. При построении словаря происходит подсчит частоты токенов. Частоты хранятся в словаре со значением по умолчанию.  Для построения словаря мы идём по всем текстам, считаем количество текстов, а также идём по всем токенам в каждом тексте и увеличиваем счётчик в нашем словаре на единицу для каждого словоупотребления. При этом происходит удаление неинформативные токенов: удаляем редкие токены (специальные слова, опечатки, идентификаторы). Токены, встретившиеся менее чем 5 раз в обучающей выборке, будут отброшены. Ещё один критерий для фильтрации — это ограничение частоты сверху т.е. убираем (союзы, предлоги, местоимения, вопросительные слова). Токены встречающиеся более, чем в 80% документов обучающей выборки будут отброшены. Далее сортируются токены по убыванию частоты их встречаемости, чтобы наиболее часто встречающиеся токены получили наименьшие идентификаторы. На этом этапе мы добавляем в список наших токенов фиктивный токен, причём добавляем его в начало. Это делается для того, чтобы он получил идентификатор "0". Помимо этого оставляем только заданное количество наиболее частотных слов. Затем назначаеются идентификаторы токенам и строим словарь в виде python dict. Функция возвращает словарь и  вектор весов, содержащий относительные частоты всех токенов в нашем датасете. Этот вектор нужен на этапе формирования матрицы признаков.
</p>


<p align="justify">
<b>vectorize_texts</b> - строит матрицу признаков по методу мешка слов. Эта функция принимает три основных параметра, а именно — список токенизированных текстов (то есть список списков строк), словарь (то есть, отображение из строк в числа) из токенов в их идентификаторы или в их номера, а также вектор, содержащий действительные числа и описывающий относительные частоты токенов. Функция принимает ещё два параметра — а именно, это алгоритм взвешивания токенов по частоте (по умолчанию выбран TF-IDF), а также флаг, который говорит, нужно ли перемасштабировать данные после взвешивания, или не нужно. Далее мы строим прямоугольную матрицу, в которой количество строк соответствует количеству текстов, а количество столбцов соответствует количеству уникальных токенов. Эта матрица будет содержать счётчики: сколько каждый токен встретился в каждом документе. Алгоритм подсчёта идёт по всем текстам, по всем токенам в каждом тексте, и если токен есть в словаре, то он увеличивает на единичку соответствующую ячейку матрицы. Далее реализуется процедуру взвешивания.
</p>

Алгоритмы взвешивания:   

* <i>Бинарные вектора</i> -матрица весов будет содержать единичку, если токен хотя бы один раз встретился в документе, и нолик, если токена в документе не было.
* <i>TF (term frequency)</i> - относительные частоты слов в документе. В этом алгоритме происходит конвертиртация в разреженную матрицу для строк и деление каждого счётчика в каждой строке на сумму элементов в этой строке.   
* <i>IDF (inverse document frequency)</i> - алгоритм взвешивания приводит к тому, что, в результирующей матрице признаков, убирается вся информация о частоте токена в документе. Но в этой матрице есть информация о частоте токена в корпусе. Здесь мы сначала получаем матрицу индикаторов (есть ли токен документе, или нет), а затем домножаем индикаторы на вектор весов слов, который мы вычислили на этапе построения словаря. 
* <i>TF-IDF</i> - сначала находим TF для каждого токена, а затем домножаем TF на IDF, затем происходит масштабирование датасета так, чтобы все элементы матрицы укладывались в диапазон от нуля до единицы. Далее, переводим разреженную матрицу в формат, ориентированный на эффективную работу со строками, потому, что алгоритмы машинного обучения, которые далее будут использоваться, читают документы один за другим и настраивают свои веса, то есть нам нужно уметь эффективно брать отдельный документ. 

Для векторизации и обучающей, и тестовой выборки, используется один и тот же набор параметров, то есть один и тот же словарь, один и тот же вектор частот и один и тот же режим векторизации.


## Технологии
* [PyTorch](https://pytorch.org/)   
* [scikit-learn](https://scikit-learn.org/stable/index.html)
* [numpy](https://numpy.org/)
* [matplotlib](https://matplotlib.org/)
