# Тематическая классификация текстов      

<p align="justify">
Будем работать с датасетом для тематической классификации, "20 новостных групп", который состоит из примерно 20 тысяч сообщений электронной почты, распределенных по 20 категориям. Датасет состоит из двух частей — из обучающей выборки и тестовой. Суммарно 18 тысяч текстов, из них примерно две трети содержатся в обучающей выборке, а треть в — тестовой.  Классы в этом датасете обозначаются числами от 0 до 19.   
</p>

## Модель    

* Модель мешка слов       
* Логистическая регрессия       

<p align="justify">
Мы  преобразуем исходный текст в токены, затем построим словарь и взвесим токены. Реализуем модель мешка слов, разреженную векторную модель текста, затем мы реализуем логистическую регрессию на pytorch, обучим её, оценим её качество. А затем мы возьмём библиотеку scikit-learn (алгоритмы векторизации текстов из неё, а также реализацию логистической регрессии), обучим этот вариант модели и сравним с нашим вариантом. 
</p>    

## Основные функции     

<p align="justify">
<b>tokenize_corpus</b> - принимает на вход список текстов (список строк), а также функцию, которая будет принимать одну строку и возвращать список токенов. В функции с помощью регулярного выражения, описывается всё возможное множество токенов, которое нас интересует, затем короткие токины удаляются. По умолчанию задана минимальная длина токена: 4. Для описания множества допустимых токенов используется регулярное выражение. Это регулярное выражение срабатывает на непрерывных последовательностях букв или цифр. Таким образом, знаки препинания, пробелы, специальные символы игнорируются. После токенизации остаются только последовательности, состоящие из букв и цифр.   
</p>

<p align="justify">
<b>build_vocabulary</b> - принимает на вход список списков. Внешний список представляет собой датасет, а внутренние списки представляют отдельные документы и элементы внутренних списков — это токены в строковом виде. При построении словаря происходит подсчит частоты токенов. Частоты хранятся в словаре со значением по умолчанию.  Для построения словаря мы идём по всем текстам, считаем количество текстов, а также идём по всем токенам в каждом тексте и увеличиваем счётчик в нашем словаре на единицу для каждого словоупотребления. При этом происходит удаление неинформативные токенов: удаляем редкие токены (специальные слова, опечатки, идентификаторы). Токены, встретившиеся менее чем 5 раз в обучающей выборке, будут отброшены. Ещё один критерий для фильтрации — это ограничение частоты сверху т.е. убираем (союзы, предлоги, местоимения, вопросительные слова). Токены встречающиеся более, чем в 80% документов обучающей выборки будут отброшены. Далее сортируются токены по убыванию частоты их встречаемости, чтобы наиболее часто встречающиеся токены получили наименьшие идентификаторы. На этом этапе мы добавляем в список наших токенов фиктивный токен, причём добавляем его в начало. Это делается для того, чтобы он получил идентификатор "0". Помимо этого оставляем только заданное количество наиболее частотных слов. Затем назначаеются идентификаторы токенам и строим словарь в виде python dict. Функция возвращает словарь и  вектор весов, содержащий относительные частоты всех токенов в нашем датасете. Этот вектор нужен на этапе формирования матрицы признаков.
</p>

##  Универсальные компоненты     

* Токенизация   
* Построение словаря    
* Фильтрация словаря   
* Построение матрицы   
* pytorch Dataset    
* цикл обучения и валидации    
* выбор лучшей модели в процессе обучения    
* оценка качества 

## Технологии
* [PyTorch](https://pytorch.org/)   
* [scikit-learn](https://scikit-learn.org/stable/index.html)
* [numpy](https://numpy.org/)
* [matplotlib](https://matplotlib.org/)
