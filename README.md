# Тематическая классификация текстов      

<p align="justify">
Будем работать с датасетом для тематической классификации, "20 новостных групп", который состоит из примерно 20 тысяч сообщений электронной почты, распределенных по 20 категориям. Датасет состоит из двух частей — из обучающей выборки и тестовой. Суммарно 18 тысяч текстов, из них примерно две трети содержатся в обучающей выборке, а треть в — тестовой.  Классы в этом датасете обозначаются числами от 0 до 19.   
</p>

## Модель    

* Модель мешка слов       
* Логистическая регрессия       

<p align="justify">
Мы  преобразуем исходный текст в токены, затем построим словарь и взвесим токены. Реализуем модель мешка слов, разреженную векторную модель текста, затем мы реализуем логистическую регрессию на pytorch, обучим её, оценим её качество. А затем мы возьмём библиотеку scikit-learn (алгоритмы векторизации текстов из неё, а также реализацию логистической регрессии), обучим этот вариант модели и сравним с нашим вариантом. 
  
Так как классы распределены почти равномерно, мы будем использовать accuracy (долю правильных предсказаний) как рабочую метрику.     
   
<b>Логистическая регрессия</b> — это линейная регрессия, выход который сжимается в диапазон от нуля до единицы с помощью логистической функции, то есть сигмоиды. Модель состоит из одного слоя (линейный слой), у которого количество входов соответствует количеству уникальных токенов, то есть размеру словаря, и количество выходов соответствует количеству меток в датасете. 
</p>    

    
##  Универсальные компоненты     

* Токенизация   
* Построение словаря    
* Фильтрация словаря   
* Построение матрицы   
* pytorch Dataset    
* цикл обучения и валидации    
* выбор лучшей модели в процессе обучения    
* оценка качества

## Основные функции и классы      

<p align="justify">
<b>tokenize_corpus</b> - принимает на вход список текстов (список строк), а также функцию, которая будет принимать одну строку и возвращать список токенов. В функции с помощью регулярного выражения, описывается всё возможное множество токенов, которое нас интересует, затем короткие токины удаляются. По умолчанию задана минимальная длина токена: 4. Для описания множества допустимых токенов используется регулярное выражение. Это регулярное выражение срабатывает на непрерывных последовательностях букв или цифр. Таким образом, знаки препинания, пробелы, специальные символы игнорируются. После токенизации остаются только последовательности, состоящие из букв и цифр.   
</p>

<p align="justify">
<b>build_vocabulary</b> - принимает на вход список списков. Внешний список представляет собой датасет, а внутренние списки представляют отдельные документы и элементы внутренних списков — это токены в строковом виде. При построении словаря происходит подсчит частоты токенов. Частоты хранятся в словаре со значением по умолчанию.  Для построения словаря мы идём по всем текстам, считаем количество текстов, а также идём по всем токенам в каждом тексте и увеличиваем счётчик в нашем словаре на единицу для каждого словоупотребления. При этом происходит удаление неинформативные токенов: удаляем редкие токены (специальные слова, опечатки, идентификаторы). Токены, встретившиеся менее чем 5 раз в обучающей выборке, будут отброшены. Ещё один критерий для фильтрации — это ограничение частоты сверху т.е. убираем (союзы, предлоги, местоимения, вопросительные слова). Токены встречающиеся более, чем в 80% документов обучающей выборки будут отброшены. Далее сортируются токены по убыванию частоты их встречаемости, чтобы наиболее часто встречающиеся токены получили наименьшие идентификаторы. На этом этапе мы добавляем в список наших токенов фиктивный токен, причём добавляем его в начало. Это делается для того, чтобы он получил идентификатор "0". Помимо этого оставляем только заданное количество наиболее частотных слов. Затем назначаеются идентификаторы токенам и строим словарь в виде python dict. Функция возвращает словарь и  вектор весов, содержащий относительные частоты всех токенов в нашем датасете. Этот вектор нужен на этапе формирования матрицы признаков.
</p>


<p align="justify">
<b>vectorize_texts</b> - строит матрицу признаков по методу мешка слов. Эта функция принимает три основных параметра, а именно — список токенизированных текстов (то есть список списков строк), словарь (то есть, отображение из строк в числа) из токенов в их идентификаторы или в их номера, а также вектор, содержащий действительные числа и описывающий относительные частоты токенов. Функция принимает ещё два параметра — а именно, это алгоритм взвешивания токенов по частоте (по умолчанию выбран TF-IDF), а также флаг, который говорит, нужно ли перемасштабировать данные после взвешивания, или не нужно. Далее мы строим прямоугольную матрицу, в которой количество строк соответствует количеству текстов, а количество столбцов соответствует количеству уникальных токенов. Эта матрица будет содержать счётчики: сколько каждый токен встретился в каждом документе. Алгоритм подсчёта идёт по всем текстам, по всем токенам в каждом тексте, и если токен есть в словаре, то он увеличивает на единичку соответствующую ячейку матрицы. Далее реализуется процедуру взвешивания.
</p>

<b>Алгоритмы взвешивания:</b>   

* <b>Бинарные вектора</b> - матрица весов будет содержать единичку, если токен хотя бы один раз встретился в документе, и нолик, если токена в документе не было.
* <b>TF (term frequency)</b> - относительные частоты слов в документе. В этом алгоритме происходит конвертиртация в разреженную матрицу для строк и деление каждого счётчика в каждой строке на сумму элементов в этой строке.   
* <b>IDF (inverse document frequency)</b> - алгоритм взвешивания приводит к тому, что, в результирующей матрице признаков, убирается вся информация о частоте токена в документе. Но в этой матрице есть информация о частоте токена в корпусе. Здесь мы сначала получаем матрицу индикаторов (есть ли токен документе, или нет), а затем домножаем индикаторы на вектор весов слов, который мы вычислили на этапе построения словаря. 
* <b>TF-IDF</b> - сначала находим TF для каждого токена, а затем домножаем TF на IDF, затем происходит масштабирование датасета так, чтобы все элементы матрицы укладывались в диапазон от нуля до единицы. Далее, переводим разреженную матрицу в формат, ориентированный на эффективную работу со строками, потому, что алгоритмы машинного обучения, которые далее будут использоваться, читают документы один за другим и настраивают свои веса, то есть нам нужно уметь эффективно брать отдельный документ. 

Для векторизации и обучающей, и тестовой выборки, используется один и тот же набор параметров, то есть один и тот же словарь, один и тот же вектор частот и один и тот же режим векторизации.

<p align="justify">
<b>SparseFeaturesDataset</b> - это класс, который принимает на вход в конструктор две матрицы — это матрица признаков, которая разрежена, и матрица меток.  Dataset реализовывает два метода — первый метод "len", он должен возвращать длину датасета (количество примеров в нём), и второй — метод "get item", он  возвращает один обучающий пример, — это вектор признаков и метка. dataset хранится в разреженном виде, но, когда нужно выбрать один пример из датасета, он выбирается из разреженной матрицы, конвертируется в плотное представление и заворачивается в "torch.Tensor". Аналогично обрабатываются метками.
</p>

<p align="justify">
<b>train_eval_loop</b> - реализует цикл обучения нейросети. Главные параметрами принимаемыми функцией являются: экземпляр  модели, обучающий датасет, валидационный датасет и функция потерь. Эта функция переносит нашу модель на то устройство, на котором мы будем производить вычисления, затем мы создаём оптимизатор (как делать градиентный шаг на каждой итерации) и настраиваем расписание изменения скорости обучения. Затем мы берём наши Dataset-ы, которые умеют возвращать обучающий пример по индексу (как мы только что рассмотрели), и передаём эти "Dataset" в "DataLoader" — это объект из pytorch, который умеет в многопоточном режиме собирать батчи примеров (чтобы всегда загружать видеокарты на 100%).    
</p>


## Эпоха обучения:  

  1. Переводим модель в режим обучения (если есть dropout или batch norm).   
  2. Делаем заданное количество градиентных шагов по обучающей выборке:   
    2.1 На каждом шаге мы берём батч примеров ( DataLoader возвращает пачки примеров). Переменная "batch_x" — это прямоугольная матрица, в которой количество строк равно количеству примеров в батче, то есть размеру батча, а количество столбцов — это количество признаков;       
    2.2 Копируем данные на то же, устройство на котором была и модель;   
    2.3 Выполняем прямой проход по модели, получаем предсказания;       
    2.4 Находим значение функции потерь (CrossEntropy);       
    2.5 Очищаем оценки градиента с предыдущего шага;        
    2.6 Находим новое значение градиентов, и делаем градиентный шаг;       
    2.7 Запоминаем среднее значение функции потерь на эпохе (для мониторинга процесса обучения).   
  3. Оценка качества модели:    
    3.1 Переводим модель в режим "eval" (режим предсказания)    
    3.2 Включаем режим "torch no_grad" (pytorch не сохраняет промежуточные данные, необходимые для вычисления градиентов)   
    3.3 Cохраняем текущий вариант модели, если среднее значение функции потерь на валидации на последней эпохе лучше, чем предыдущее лучшее значение функции потерь.    
    3.4 Прекращаем обучение, если давно не получалось улучшить модель    
  4. Обновляем скорость обучения
  5. Функция train_eval_loop возвращает лучшее значение функции потерь и модель с лучшими весами



## Технологии
* [PyTorch](https://pytorch.org/)   
* [scikit-learn](https://scikit-learn.org/stable/index.html)
* [numpy](https://numpy.org/)
* [matplotlib](https://matplotlib.org/)
