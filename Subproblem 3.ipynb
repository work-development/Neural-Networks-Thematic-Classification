{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d8a8e",
   "metadata": {},
   "source": [
    "### Матрица признаков для текстов       \n",
    "\n",
    "Построим матрицу признаков для текстов  с использованием словаря и вектора весов, полученного ранее. Будем использовать взвешивание \n",
    "$$lTFIDF = \\ln(TF + 1) \\cdot IDF$$\n",
    "\n",
    "Значения признаков будим маштабировать так, чтобы для каждого признака его среднее значение по выборке равнялось 0, а среднеквадратичное отклонение \n",
    "1: \n",
    "$$x^{scaled}_{i} = \\frac{x_{i} - E(x)} {\\sigma(x)}$$\n",
    "\n",
    "\n",
    "В результате масштабирования для каждого столбца матрицы признаков среднее будет равняться 0, а среднеквадратичное отклонение 1. При расчёте среднеквадратического отклонения будем использовать скорректированную оценку \n",
    "\n",
    "$$\\sigma=\\sqrt{\\frac{\\sum_{i-1}^n(x_i - E(x))^2}{n - 1}}$$.\n",
    "\n",
    "Чтобы получить такую оценку с помощью numpy, необходимо передать параметр ddof=1: \n",
    "\n",
    "feature_matrix = np.zeros((num_docs, num_feats))    \n",
    "feats_std = feature_matrix.std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6af67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "from dlnlputils.data import build_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70ba8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "def tokenize_text_simple_regex(txt, min_token_size=1):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d80ae5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "нельзя не помиловать\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Казнить нельзя, помиловать. Нельзя наказывать.\", \"Казнить, нельзя помиловать. Нельзя освободить.\", \"Нельзя не помиловать.\", \"Обязательно освободить.\"]\n",
    "texts_tokenized = tokenize_corpus(texts)\n",
    "print(' '.join(texts_tokenized[2]))\n",
    "MAX_DF = 1\n",
    "MIN_COUNT = 0\n",
    "vocab, w_doc_freq = build_vocabulary(texts_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d06f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'помиловать': 0.75,\n",
       " 'нельзя': 0.75,\n",
       " 'казнить': 0.5,\n",
       " 'освободить': 0.5,\n",
       " 'наказывать': 0.25,\n",
       " 'не': 0.25,\n",
       " 'обязательно': 0.25}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = {x[0]: w_doc_freq[x[1]] for x in vocab.items()}\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fe6e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('наказывать', 0.25), ('не', 0.25), ('обязательно', 0.25), ('казнить', 0.5), ('освободить', 0.5), ('нельзя', 0.75), ('помиловать', 0.75)]\n"
     ]
    }
   ],
   "source": [
    "e = sorted(dd.items(), key=lambda pair: (pair[1],pair[0]))\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9d4b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'помиловать': 6,\n",
       " 'нельзя': 5,\n",
       " 'казнить': 3,\n",
       " 'освободить': 4,\n",
       " 'наказывать': 0,\n",
       " 'не': 1,\n",
       " 'обязательно': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab = {}\n",
    "for x in list(vocab.keys()):\n",
    "    for i,y in enumerate(e):\n",
    "        if x == y[0]:\n",
    "            new_vocab[x] = i\n",
    "\n",
    "new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3882c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03571428571428571\n",
      "0.03571428571428571\n",
      "0.03571428571428571\n",
      "0.07142857142857142\n",
      "0.07142857142857142\n",
      "0.10714285714285714\n",
      "0.10714285714285714\n"
     ]
    }
   ],
   "source": [
    "for _,w in e:\n",
    "    print(w/len(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5d1578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lTFIDF=ln(TF+1)⋅IDF.\n",
    "import scipy\n",
    "def vectorize_texts_MY(tokenized_texts, word2id, word2freq, mode='tfidf', scale=True):\n",
    "    assert mode in {'tfidf', 'idf', 'tf', 'bin'}\n",
    "\n",
    "    # считаем количество употреблений каждого слова в каждом документе\n",
    "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
    "    for text_i, text in enumerate(tokenized_texts):\n",
    "        for token in text:\n",
    "            if token in word2id:\n",
    "                result[text_i, word2id[token]] += 1\n",
    "\n",
    "    # получаем бинарные вектора \"встречается или нет\"\n",
    "    if mode == 'bin':\n",
    "        result = (result > 0).astype('float32')\n",
    "\n",
    "    # получаем вектора относительных частот слова в документе\n",
    "    elif mode == 'tf':\n",
    "        result = result.tocsr()\n",
    "        result = result.multiply(1 / result.sum(1))\n",
    "\n",
    "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
    "    # но оставляем информацию о частотности слова в корпусе в целом\n",
    "    elif mode == 'idf':\n",
    "        result = (result > 0).astype('float32').multiply(1 / word2freq)\n",
    "\n",
    "    # учитываем всю информацию, которая у нас есть:\n",
    "    # частоту слова в документе и частоту слова в корпусе\n",
    "    elif mode == 'tfidf':\n",
    "        result = result.tocsr().toarray()\n",
    "        result = np.log(result*(1 / result.sum(1)).reshape([4,1]) + 1)  # разделить каждую строку на её длину\n",
    "        result = result*(1 / word2freq)  # разделить каждый столбец на вес слова\n",
    "\n",
    "    # if scale:\n",
    "    #     result = result.tocsc()\n",
    "    #     result -= result.min()\n",
    "    #     result /= (result.max() + 1e-6)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af58e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTORIZATION_MODE = 'tfidf'\n",
    "word2freq = w_doc_freq[::-1]\n",
    "texts_vectors = vectorize_texts_MY(texts_tokenized, new_vocab, word2freq, mode=VECTORIZATION_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aeeb721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'],\n",
       " ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'],\n",
       " ['нельзя', 'не', 'помиловать'],\n",
       " ['обязательно', 'освободить']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63b5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = texts_vectors\n",
    "L = (arr - arr.mean(axis=0))/arr.std(0, ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac5382fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18232161, 0.28768212, 0.4054651 , 0.18232161, 0.29389334,\n",
       "       0.32020888, 0.21744178], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d348f66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 -0.5 -0.5 0.86602545 -0.7630126 0.5954669 0.16096799\n",
      "-0.5 -0.5 -0.5 0.86602545 0.18368244 0.5954669 0.16096799\n",
      "-0.5 1.5 -0.5 -0.86602545 -0.7630126 0.29382402 1.0424348\n",
      "-0.5 -0.5 1.5 -0.86602545 1.3423429 -1.4847578 -1.3643708\n"
     ]
    }
   ],
   "source": [
    "# ОТВЕТ\n",
    "for i in range(L.shape[0]):\n",
    "    print(' '.join(map(str,L[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b852165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18232161, 0.28768212, 0.4054651 , 0.18232161, 0.29389334,\n",
       "       0.32020888, 0.21744178], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e408ba82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix = np.zeros((len(L), len(L[0])))\n",
    "np.array([4,4])/np.array([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7dd26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
